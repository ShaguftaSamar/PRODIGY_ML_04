{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7930e691",
   "metadata": {},
   "source": [
    "Task-04: Develop a hand gesture recognition model that can accurately identify and classify different hand gestures from image or video data, enabling intuitive human-computer interaction and gesture-based control systems.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e86262",
   "metadata": {},
   "source": [
    "Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7466ae31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎉 Smaller dataset created with 200 images per gesture at: C:\\Users\\Shagufta Umme\\Desktop\\Hand Gesture\\leapGestRecog_small\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "\n",
    "# Source dataset\n",
    "source_dir = r\"C:\\Users\\Shagufta Umme\\Desktop\\Hand Gesture\\leapGestRecog\"\n",
    "\n",
    "# Target smaller dataset\n",
    "target_dir = r\"C:\\Users\\Shagufta Umme\\Desktop\\Hand Gesture\\leapGestRecog_small\"\n",
    "\n",
    "# Limit per gesture\n",
    "num_samples_per_class = 200  \n",
    "\n",
    "# Correct mapping (from your dataset)\n",
    "gesture_map = {\n",
    "    \"01_palm\": \"Palm\",\n",
    "    \"02_l\": \"L\",\n",
    "    \"03_fist\": \"Fist\",\n",
    "    \"04_fist_moved\": \"FistMoved\",\n",
    "    \"05_thumb\": \"Thumb\",\n",
    "    \"06_index\": \"Index\",\n",
    "    \"07_ok\": \"OK\",\n",
    "    \"08_palm_moved\": \"PalmMoved\",\n",
    "    \"09_c\": \"C\",\n",
    "    \"10_down\": \"Down\"\n",
    "}\n",
    "\n",
    "os.makedirs(target_dir, exist_ok=True)\n",
    "\n",
    "# Dictionary to collect all images of each gesture\n",
    "gesture_images = {g: [] for g in gesture_map.keys()}\n",
    "\n",
    "# Step 1: Gather all image paths across users\n",
    "for user_folder in os.listdir(source_dir):  # loop through 00–09 users\n",
    "    user_path = os.path.join(source_dir, user_folder)\n",
    "    if not os.path.isdir(user_path):\n",
    "        continue\n",
    "    \n",
    "    for gesture_folder in os.listdir(user_path):  # loop gesture subfolders\n",
    "        if gesture_folder not in gesture_map:\n",
    "            continue\n",
    "        \n",
    "        gesture_path = os.path.join(user_path, gesture_folder)\n",
    "        if not os.path.isdir(gesture_path):\n",
    "            continue\n",
    "        \n",
    "        all_imgs = [os.path.join(gesture_path, f) for f in os.listdir(gesture_path) if f.endswith('.png')]\n",
    "        gesture_images[gesture_folder].extend(all_imgs)\n",
    "\n",
    "# Step 2: For each gesture, sample only 200 images (total, across all users)\n",
    "for gesture_folder, img_list in gesture_images.items():\n",
    "    gesture_name = gesture_map[gesture_folder]\n",
    "    target_class_dir = os.path.join(target_dir, gesture_name)\n",
    "    os.makedirs(target_class_dir, exist_ok=True)\n",
    "\n",
    "    selected_files = random.sample(img_list, min(num_samples_per_class, len(img_list)))\n",
    "\n",
    "    for idx, src in enumerate(selected_files):\n",
    "        dst = os.path.join(target_class_dir, f\"{gesture_folder}_{idx:04d}.png\")\n",
    "        shutil.copy(src, dst)\n",
    "\n",
    "print(\"🎉 Smaller dataset created with 200 images per gesture at:\", target_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a48bdc",
   "metadata": {},
   "source": [
    "Model Training & Real-Time Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f118c68c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1600 images belonging to 10 classes.\n",
      "Found 400 images belonging to 10 classes.\n",
      "Labels: ['C', 'Down', 'Fist', 'FistMoved', 'Index', 'L', 'OK', 'Palm', 'PalmMoved', 'Thumb']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">62</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">62</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">31</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">31</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">29</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">29</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12544</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,605,760</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,290</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m62\u001b[0m, \u001b[38;5;34m62\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │           \u001b[38;5;34m320\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_2 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m31\u001b[0m, \u001b[38;5;34m31\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_3 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m29\u001b[0m, \u001b[38;5;34m29\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │        \u001b[38;5;34m18,496\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_3 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_1 (\u001b[38;5;33mFlatten\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12544\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │     \u001b[38;5;34m1,605,760\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │         \u001b[38;5;34m1,290\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,625,866</span> (6.20 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,625,866\u001b[0m (6.20 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,625,866</span> (6.20 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,625,866\u001b[0m (6.20 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 88ms/step - accuracy: 0.2640 - loss: 2.0888 - val_accuracy: 0.7900 - val_loss: 0.9073\n",
      "Epoch 2/10\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 72ms/step - accuracy: 0.6945 - loss: 0.9227 - val_accuracy: 0.8950 - val_loss: 0.3983\n",
      "Epoch 3/10\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 77ms/step - accuracy: 0.8871 - loss: 0.4083 - val_accuracy: 0.9575 - val_loss: 0.1973\n",
      "Epoch 4/10\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 76ms/step - accuracy: 0.9273 - loss: 0.2502 - val_accuracy: 0.9775 - val_loss: 0.1239\n",
      "Epoch 5/10\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 76ms/step - accuracy: 0.9504 - loss: 0.1634 - val_accuracy: 0.9800 - val_loss: 0.1151\n",
      "Epoch 6/10\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 71ms/step - accuracy: 0.9763 - loss: 0.0903 - val_accuracy: 0.9825 - val_loss: 0.1004\n",
      "Epoch 7/10\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 80ms/step - accuracy: 0.9700 - loss: 0.0932 - val_accuracy: 0.9800 - val_loss: 0.1002\n",
      "Epoch 8/10\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 80ms/step - accuracy: 0.9783 - loss: 0.0717 - val_accuracy: 0.9850 - val_loss: 0.0953\n",
      "Epoch 9/10\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 76ms/step - accuracy: 0.9786 - loss: 0.0663 - val_accuracy: 0.9825 - val_loss: 0.0841\n",
      "Epoch 10/10\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 81ms/step - accuracy: 0.9828 - loss: 0.0624 - val_accuracy: 0.9850 - val_loss: 0.1022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: Fist (99.54% confidence)\n",
      "Webcam released and windows closed.\n"
     ]
    }
   ],
   "source": [
    "# 1. Import Libraries\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# 2. Dataset Path\n",
    "data_dir = r\"C:\\Users\\Shagufta Umme\\Desktop\\Hand Gesture\\leapGestRecog_small\"\n",
    "\n",
    "# 3. Data Preprocessing\n",
    "IMG_SIZE = (64, 64)\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)\n",
    "\n",
    "train_gen = datagen.flow_from_directory(\n",
    "    data_dir,\n",
    "    target_size=IMG_SIZE,\n",
    "    color_mode=\"grayscale\",\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode=\"categorical\",\n",
    "    subset=\"training\",\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "val_gen = datagen.flow_from_directory(\n",
    "    data_dir,\n",
    "    target_size=IMG_SIZE,\n",
    "    color_mode=\"grayscale\",\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode=\"categorical\",\n",
    "    subset=\"validation\",\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "labels = list(train_gen.class_indices.keys())\n",
    "print(\"Labels:\", labels)\n",
    "\n",
    "# 4. CNN Model\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3,3), activation='relu', input_shape=(64,64,1)),\n",
    "    MaxPooling2D((2,2)),\n",
    "\n",
    "    Conv2D(64, (3,3), activation='relu'),\n",
    "    MaxPooling2D((2,2)),\n",
    "\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(len(labels), activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer=Adam(0.001), loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "model.summary()\n",
    "\n",
    "# 5. Training\n",
    "history = model.fit(\n",
    "    train_gen,\n",
    "    epochs=10,\n",
    "    validation_data=val_gen\n",
    ")\n",
    "\n",
    "# Save model\n",
    "model.save(\"gesture_cnn_model.h5\")\n",
    "\n",
    "# 6. Prediction on Single Image\n",
    "def predict_image(img_path):\n",
    "    img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "    img = cv2.resize(img, IMG_SIZE)\n",
    "    img = img.astype(\"float32\") / 255.0\n",
    "    img = np.expand_dims(img, axis=-1)\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "\n",
    "    prediction = model.predict(img)\n",
    "    class_id = np.argmax(prediction)\n",
    "    confidence = prediction[0][class_id]\n",
    "    return labels[class_id], confidence\n",
    "\n",
    "# Example usage\n",
    "test_img = r\"C:\\Users\\Shagufta Umme\\Desktop\\Hand Gesture\\leapGestRecog_small\\Fist\\03_fist_0010.png\"\n",
    "gesture, conf = predict_image(test_img)\n",
    "print(f\"Predicted: {gesture} ({conf*100:.2f}% confidence)\")\n",
    "\n",
    "# 7. Real-Time Webcam Prediction (Hand Only)\n",
    "def real_time_prediction_with_box(model_path=\"gesture_cnn_model.h5\", box_size=200):\n",
    "    model = load_model(model_path)\n",
    "    cap = cv2.VideoCapture(0)\n",
    "\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not open webcam.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                print(\"Error: Could not read frame.\")\n",
    "                break\n",
    "\n",
    "            h, w, _ = frame.shape\n",
    "            x1 = w//2 - box_size//2\n",
    "            y1 = h//2 - box_size//2\n",
    "            x2 = w//2 + box_size//2\n",
    "            y2 = h//2 + box_size//2\n",
    "\n",
    "            # Crop ROI (hand only)\n",
    "            roi = frame[y1:y2, x1:x2]\n",
    "            if roi.size == 0:\n",
    "                continue\n",
    "\n",
    "            # Preprocess for prediction\n",
    "            gray = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)\n",
    "            img = cv2.resize(gray, IMG_SIZE)\n",
    "            img = img.astype(\"float32\") / 255.0\n",
    "            img = np.expand_dims(img, axis=-1)\n",
    "            img = np.expand_dims(img, axis=0)\n",
    "\n",
    "            # Predict gesture\n",
    "            prediction = model.predict(img, verbose=0)\n",
    "            class_id = np.argmax(prediction)\n",
    "            confidence = prediction[0][class_id]\n",
    "            label = labels[class_id]\n",
    "\n",
    "            # Draw bounding box on ROI\n",
    "            cv2.rectangle(roi, (0, 0), (box_size-1, box_size-1), (0, 255, 0), 2)\n",
    "            cv2.putText(roi, f\"{label} ({confidence*100:.1f}%)\", (5, 25),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2)\n",
    "\n",
    "            # Display only the hand ROI (resized for better view)\n",
    "            display_roi = cv2.resize(roi, (300, 300))\n",
    "            cv2.imshow(\"Gesture Recognition (Hand Only)\", display_roi)\n",
    "\n",
    "            # Quit on 'q'\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "\n",
    "    finally:\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "        print(\"Webcam released and windows closed.\")\n",
    "\n",
    "# To run real-time hand gesture recognition\n",
    "real_time_prediction_with_box()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0220d242",
   "metadata": {},
   "source": [
    "Model Evaluation & Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f1efebd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step - accuracy: 0.9911 - loss: 0.0845\n",
      "Validation Accuracy: 98.75%\n",
      "\n",
      "Confusion Matrix (Validation Set):\n",
      " [[40  0  0  0  0  0  0  0  0  0]\n",
      " [ 0 40  0  0  0  0  0  0  0  0]\n",
      " [ 0  0 40  0  0  0  0  0  0  0]\n",
      " [ 1  0  0 39  0  0  0  0  0  0]\n",
      " [ 0  0  0  0 38  2  0  0  0  0]\n",
      " [ 0  0  0  0  0 39  0  1  0  0]\n",
      " [ 0  0  0  0  0  0 40  0  0  0]\n",
      " [ 0  0  0  0  0  0  0 40  0  0]\n",
      " [ 0  0  0  1  0  0  0  0 39  0]\n",
      " [ 0  0  0  0  0  0  0  0  0 40]]\n",
      "\n",
      "Classification Report (Validation Set):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           C       0.98      1.00      0.99        40\n",
      "        Down       1.00      1.00      1.00        40\n",
      "        Fist       1.00      1.00      1.00        40\n",
      "   FistMoved       0.97      0.97      0.97        40\n",
      "       Index       1.00      0.95      0.97        40\n",
      "           L       0.95      0.97      0.96        40\n",
      "          OK       1.00      1.00      1.00        40\n",
      "        Palm       0.98      1.00      0.99        40\n",
      "   PalmMoved       1.00      0.97      0.99        40\n",
      "       Thumb       1.00      1.00      1.00        40\n",
      "\n",
      "    accuracy                           0.99       400\n",
      "   macro avg       0.99      0.99      0.99       400\n",
      "weighted avg       0.99      0.99      0.99       400\n",
      "\n",
      "\n",
      "No test set folder found. Skipping test accuracy calculation.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# 1. Validation Set Metrics\n",
    "val_gen.reset()  # Make sure validation generator starts from the first batch\n",
    "loss, val_accuracy = model.evaluate(val_gen)\n",
    "print(f\"Validation Accuracy: {val_accuracy*100:.2f}%\\n\")\n",
    "\n",
    "# Predict on validation set\n",
    "predictions = model.predict(val_gen, verbose=0)\n",
    "y_pred = np.argmax(predictions, axis=1)\n",
    "y_true = val_gen.classes\n",
    "\n",
    "# Class labels\n",
    "class_labels = list(val_gen.class_indices.keys())\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "print(\"Confusion Matrix (Validation Set):\\n\", cm)\n",
    "\n",
    "# Classification report\n",
    "report = classification_report(y_true, y_pred, target_names=class_labels)\n",
    "print(\"\\nClassification Report (Validation Set):\\n\", report)\n",
    "\n",
    "# ------------------------------\n",
    "# 2. Test Set Accuracy (Optional)\n",
    "# ------------------------------\n",
    "test_dir = r\"C:\\Users\\Shagufta Umme\\Desktop\\Hand Gesture\\TestSet\"  # Update path if needed\n",
    "if os.path.exists(test_dir):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for gesture in os.listdir(test_dir):\n",
    "        gesture_dir = os.path.join(test_dir, gesture)\n",
    "        for img_name in os.listdir(gesture_dir):\n",
    "            img_path = os.path.join(gesture_dir, img_name)\n",
    "            pred_label, _ = predict_image(img_path)\n",
    "            if pred_label == gesture:\n",
    "                correct += 1\n",
    "            total += 1\n",
    "    test_accuracy = correct / total if total > 0 else 0\n",
    "    print(f\"\\nTest Set Accuracy: {test_accuracy*100:.2f}%\")\n",
    "else:\n",
    "    print(\"\\nNo test set folder found. Skipping test accuracy calculation.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10501d24",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
